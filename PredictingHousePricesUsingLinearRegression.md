# Predicting House Prices Using Linear Regression

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Regardless of if you are contemplating selling your house, or if you are saving up to buy your dream home, you are probably interested in how much a similar house in a similar area is worth. In this paper I will describe the process of predicting future house prices, using linear regression. It will be loosely based on the checklist for machine learning projects, described by Aurélien Géron.<a href="#Geron(2017)" id="note1ref"><sup>1</sup></a>

## Frame the problem

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Multiple linear regression, which falls into the category supervised machine learning,<a href="#Pant(2019)" id="note2ref"><sup>2</sup></a> will be used to predict the house prices. Using linear regression, previous research<a href="#deCock(2011)" id="note3ref"><sup>3</sup></a> has been able to explain between 80% (two independent variables) and 92% (36 independent variables) of the variation in price. In this project, it would therefore be reasonable to aim to explain somewhere between 85-90% of the variance. I will focus on how to create the model in Python, although it would also be possible to use several other programming languages, such as R.

## Gathering the Data

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;There are already several available datasets for predicting house prices, e.g. the Boston Housing Dataset<a href="#Harrison&Rubinfeld(1978)" id="note4ref"><sup>4</sup></a> and the Ames Iowa Housing Data,<a href="#deCock(2011)" id="note3ref"><sup>3</sup></a> which include variables such as: number of rooms, age of the house, proximity to larger roads,<a href="#deCock(2011)" id="note3ref"><sup>3,</sup></a> <a href="#Harrison&Rubinfeld(1978)" id="note4ref"><sup>4 </sup></a>and per capita crime rate.<a href="#Harrison&Rubinfeld(1978)" id="note4ref"><sup>4</sup></a> In the same manner as de Cock,<a href="#deCock(2011)" id="note3ref"><sup>3</sup></a> when he got access to the data by contacting the Ames City Assessor’s Office, I would suggest collecting the data from an authority, since it will be less biased, compared to for example data collected from a broker. In a Swedish context, Statistiska Centralbyrån holds classified microdata about prices and characteristics of houses in Sweden,<a href="#StatistiskaCentralbyran(2021)a" id="note5ref"><sup>5</sup></a> which one can request to get access to in anonymised form.<a href="#StatistiskaCentralbyran(2021)b" id="note6ref"><sup>6</sup></a> We would ideally have a sample size of *N* ≥ 50 + 8*m* (*m* is the number of independent variables) for testing the full model and *N* ≥ 104 + *m* for testing the indvidual predictors.<a href="#Tabachnick&Fidell(2014)" id="note7ref"><sup>7</sup></a> \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Once the data is collected I would recommend to initially save the data in an anonymised form in a csv-file, which is commonly used for machine learning projects with tabular data.<a href="#Dowling(2019)" id="note8ref"><sup>8</sup></a> The data can then be stored in the cloud, using for example Amazon SageMaker.<a href="#AmazonWebServices,Inc(2019)" id="note9ref"><sup>9</sup></a> It is also important to follow local regulations for data storage, for example following GDPR<a href="#EuropeanCommision(n.d.)" id="note10ref"><sup>10</sup></a> if the data is collected in Europe.

## Exploring and preparing the data

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Once we have collected our data, we will do some initial exploration. We can use Matplotlib (reference) to create simple boxplots for visually identifying outliers and to make scatterplots for getting an overview of the linear relationship between the variables. \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In the next step we have to prepare the raw data for analysis by cleaning it, using for example Pandas (reference) or NumPy (reference). If the dataset contains missing values we can either remove the whole row (however this is not recommended if the dataset contains a lot of missing values), or we can replace the missing value, e.g. by using the mean (Pant, 2019). We should also remove previously identified outliers and check for collinearity/multicollinearity, i.e. that two or more of our independent variables are linearly related and only keep the one that did the best job at predicting the price (reference). If the data is not normally distributed we have to transform it to a normal distribution, using for example a log-transformation. If the dataset contains discrete values they can be converted to a set of dichotomoues values, using dummy-variable coding (e.g. no garage = 0, garage = 1). (Tabachnick & Fidell, 2014).  We can also scale our variables, so that our results are easier to interpret (reference). \
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Depending on the package to be used to analyse the data, we might have to change it to a NumPy array (reference). The dataset should be randomly split into training for training the model, validation for tuning the parameters and testing for testing the model on unseen data (Pant, 2019). However, we will not use the validation dataset if we create the model using an analytical solution (book).  A common way to split the data is to use 25-30% for testing and 70-75% for training (Mueller, Massaron). If the dataset is small it is good practice to use cross-validation, which runs the model numerous times on different subsets of the data. (Becker and Cook). train_test_split

## References
<a id="Geron(2017)" href="#note1ref">1</sup></a>. Géron, A. (2017).
*Hands-on machine learning with scikit-learn, Keras, and TensorFlow*. O'Reilly Media, Inc. \
<a id="Pant(2019)" href="#note2ref">2</sup></a>. Pant, A. (2019, January 11). Workflow of a machine learning project. *Towards Data Science.* https://towardsdatascience.com/workflow-of-a-machine-learning-project-ec1dba419b94 \
<a id="deCock(2011)" href="#note3ref">3</sup></a>. de Cock, D. (2011). Ames, Iowa: Alternative to the Boston Housing Data
as an end of semester regression project. *Journal of Statistics Education, 19*(3), 1-15. https://doi.org/10.1080/10691898.2011.11889627 \
<a id="Harrison&Rubinfeld(1978)" href="#note4ref">4</sup></a>. Harrison, D. & Rubinfeld, D. L. (1978). Hedonic housing prices and the demand for clean air. *Journal of Environmental Economics and Management, 5*(1), 81-102. https://doi.org/10.1016/0095-0696(78)90006-2 \
<a id="StatistiskaCentralbyran(2021)a" href="#note5ref">5</sup></a>. Statistiska Centralbyrån. (n.d.). *SCB:s mikrodataregister.* Retrieved September 01, 2021, from https://www.h6.scb.se/metadata/mikrodataregister.aspx \
<a id="StatistiskaCentralbyran(2021)b" href="#note6ref">6</sup></a>. Statistiska Centralbyrån. (n.d.). *Beställa mikrodata.* Retrieved September 01, 2021, from https://scb.se/vara-tjanster/bestall-data-och-statistik/bestalla-mikrodata/ \
<a id="Tabachnick&Fidell(2014)" href="#note7ref">7</sup></a>. Tabachnick, B.G. & Fidell, L.S. (2014). *Using multivariate statistics*. Pearson Education Limited. \
<a id="Dowling(2019)" href="#note8ref">8</sup></a>. "Dowling, J. (2019, October 25). Guide to file formats for machine learning: columnar, training, inferencing, and the feature store. *Towards Data Science.* https://towardsdatascience.com/guide-to-file-formats-for-machine-learning-columnar-training-inferencing-and-the-feature-store-2e0c3d18d4f9 \
<a id="AmazonWebServices,Inc(2019)" href="#note9ref">9</sup></a>. Amazon Web Services, Inc. (2019). *Amazon SageMaker, Machine learning for every data scientist and developer.* https://aws.amazon.com/sagemaker/ \
<a id="EuropeanCommision(n.d.)" href="#note10ref">10</sup></a>. European Commision. (n.d.). Data protection in the EU, The General Data Protection Regulation (GDPR), the Data Protection Law Enforcement Directive and other rules concerning the protection of personal data. Retrieved September 01, 2021, from https://ec.europa.eu/info/law/law-topic/data-protection/data-protection-eu_en